# Databricks notebook source
# MAGIC %md
# MAGIC # ALCOHOL AND GMS/PMS PUBLICATION JOB
# MAGIC 
# MAGIC This job outputs four quarterly CSVs for the entire financial year after suppression applied 

# COMMAND ----------

# MAGIC %md
# MAGIC ###CONTENTS
# MAGIC <ol>
# MAGIC   <li>Getting data from the database</li>
# MAGIC   <li>Testing outputs</li>
# MAGIC   <li>Suppresssion</li>
# MAGIC   <li>Quarterly Split</li>
# MAGIC   <li>Spot checks on few practices</li>
# MAGIC   <li>Addendum : Suppresssion rule 3</li>
# MAGIC </ol>

# COMMAND ----------

import pyspark.sql.functions as F
from pyspark.sql.functions import substring_index,substring,count,countDistinct,col , when
import pandas as pd
import datetime
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, FloatType, LongType
from pyspark.sql import DataFrame

# COMMAND ----------

# MAGIC %run ./parameters

# COMMAND ----------

## I cant write text or log files in DAE, so I will text out print statements which I would have stuck into logs

# import logging
# logging.basicConfig(filename='functions/changelog.log', level=logging.INFO)
# logging.info("This is a info message")

# with open('myfile.txt', 'w') as file:
#     file.write("Hello, world!\n")
#     file.write("How are you today?")

#Instead I will create a logging table. Can remove the print(logging_df) lines across the main file once DAE allows us to write to a txt
import time
logging_data = {"Time" : [], "Log_Type" : [], "Message" : []}
logging_df = pd.DataFrame(logging_data)


def log_info(report_msg : str) -> str:
  
    """Gets the current time. 
    Parameters: report_msg - A string about the report message
    Returns: now - The current time as a string"""
    
    now = time.strftime("%d/%m/%Y %H:%M")
    logging_df.loc[len(logging_df)] = [now, "Info", report_msg]
    return now
    
def log_test(report_msg : str) -> str:
  
    """Gets the current time. 
    Parameters: report_msg - A string about the report message
    Returns: now - The current time as a string"""
    
    now = time.strftime("%d/%m/%Y %H:%M")
    logging_df.loc[len(logging_df)] = [now, "Test", report_msg]
    return now

log_info("report start = " + report_start)
log_info("report end = " + report_end)
log_info("service year = " + service_year)

#print(logging_df)

# COMMAND ----------

dbutils.widgets.removeAll()

# COMMAND ----------

#create widget for data report time
dbutils.widgets.text("report_start", defaultValue= report_start , label="Reporting Period Start Date")
dbutils.widgets.text("report_end", defaultValue= report_end, label="Reporting Period End Date")
dbutils.widgets.text("service_year", defaultValue= service_year, label="Service Year")

log_info("Successfully set the widget time texts")

#settings
report_period_start = pd.to_datetime(report_start)
report_period_end = pd.to_datetime(report_end)
quality_service_name = f"Core GP Contract {service_year}"

# COMMAND ----------

# MAGIC %md
# MAGIC ### Input Checks

# COMMAND ----------

# MAGIC %run ./tests/input_checks

# COMMAND ----------

print(logging_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Run all functions needed

# COMMAND ----------

# MAGIC %run ./functions/run_functions

# COMMAND ----------

# MAGIC %md
# MAGIC ###Data Ingestion

# COMMAND ----------

final_df = data_ingestion(print_tables=False)

# COMMAND ----------

print(logging_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Unit Tests for Testing and Suppression

# COMMAND ----------

# MAGIC %run ./tests/function_test_suite

# COMMAND ----------

# MAGIC %run ./tests/testing_data

# COMMAND ----------

print(logging_df)

# COMMAND ----------

# MAGIC %run ./tests/test_suppression

# COMMAND ----------

print(logging_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Suppression

# COMMAND ----------

main_table, main_supp_out = apply_suppression(final_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Quartely Split

# COMMAND ----------

quartely_split(main_supp_out, split_quarters = True)

# COMMAND ----------

# MAGIC %md
# MAGIC ### QA TESTS

# COMMAND ----------

# MAGIC %run ./tests/QA_checks

# COMMAND ----------

display(logging_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Spot Checks

# COMMAND ----------

#Compare spot checked Spark DataFrames before and after suppression. These tables are randomly generated by picking 4 random suppressed practice_codes.
before_suppression_test_df, after_suppresion_test_df = spot_check(main_table, main_supp_out, display_table = True)